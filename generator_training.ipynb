{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ASz3QctH14jY"
      },
      "outputs": [],
      "source": [
        "!pip install kora q\n",
        "import kora.install.rdkit"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "metadata": {
        "id": "31B4UzU12QQ5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Vf7vGr0Q2tCF"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "sys.path.append('/content/gdrive/MyDrive/code/generator')\n",
        "sys.path.append('/content/gdrive/MyDrive/code/predictor')\n",
        "sys.path.append('/content/gdrive/MyDrive/code/reinforce')\n",
        "sys.path.append('/content/gdrive/MyDrive/code/result')\n",
        "sys.path.append('/content/gdrive/MyDrive/code/dataset')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OMT6nUa8u40W"
      },
      "outputs": [],
      "source": [
        "import argparse\n",
        "import os\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import random\n",
        "import sys\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from tqdm import tqdm\n",
        "from rdkit import Chem\n",
        "from models import RNN, OneHotRNN, EarlyStopping\n",
        "from datasets import SmilesDataset, SelfiesDataset, SmilesCollate\n",
        "from functions import decrease_learning_rate, print_update, track_loss, \\\n",
        "     sample_smiles, write_smiles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iZFMEVug14jb"
      },
      "outputs": [],
      "source": [
        "## seed all RNGs\n",
        "seed = 0    # Mention seed value\n",
        "torch.manual_seed(seed)\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "if torch.cuda.is_available():\n",
        "    print(\"using cuda\")\n",
        "    torch.cuda.manual_seed_all(seed)\n",
        "    \n",
        "\n",
        "# suppress Chem.MolFromSmiles error output\n",
        "from rdkit import rdBase\n",
        "rdBase.DisableLog('rdApp.error')\n",
        "\n",
        "    \n",
        "#Output directory\n",
        "output_dir = '/content/result'\n",
        "\n",
        "# make output directories\n",
        "if not os.path.isdir(output_dir):\n",
        "    try:\n",
        "        os.makedirs(output_dir)\n",
        "    except FileExistsError:\n",
        "        pass\n",
        "\n",
        "# sample a set of SMILES from the final, trained model\n",
        "sample_size = 10000          # (type=int, default=100000)\n",
        "batch_size = 128              # (type=int, default=128)\n",
        "\n",
        "dataset = SmilesDataset(smiles_file='/content/gdrive/MyDrive/code/generator/pre-trained/chembl_500000.csv') # Dataset file name\n",
        "\n",
        "\n",
        "# set up batching\n",
        "\n",
        "batch_size = 128\n",
        "loader = DataLoader(dataset,\n",
        "                    batch_size=batch_size,\n",
        "                    shuffle=True,\n",
        "                    drop_last=True,\n",
        "                    collate_fn=SmilesCollate(dataset.vocabulary))\n",
        "\n",
        "\n",
        "model = RNN(vocabulary=dataset.vocabulary,\n",
        "                rnn_type='GRU',                      # str; RNN type choices=['RNN', 'LSTM', 'GRU']\n",
        "                embedding_size= 128,                 # int; embedding size\n",
        "                hidden_size=512,                     # int; size of language model hidden layers\n",
        "                n_layers=3,                          # int; number of layers in language model\n",
        "                dropout=0,                           # float; amount of dropout (0-1) to apply to model\n",
        "                bidirectional=False,                 # bool; for LSTMs only, train a bidirectional mode\n",
        "                tie_weights=False,\n",
        "                nonlinearity='tanh')\n",
        "\n",
        "\n",
        "# set up optimizer\n",
        "\n",
        "\n",
        "# optimization parameters\n",
        "learning_rate = 0.001   # initial learning rate\n",
        "learning_rate_decay = None   #amount (0-1) to decrease learning rate by every ' + \\ 'fixed number of steps')\n",
        "learning_rate_decay_steps = 10000       # Number of steps between learning rate decrements\n",
        "log_every_epochs = 1000     #log training/validation losses every n epochs\n",
        "\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(),\n",
        "                       betas=(0.9, 0.999), ## default\n",
        "                       eps=1e-08, ## default\n",
        "                       lr=learning_rate)\n",
        "\n",
        "\n",
        "# Print model's state_dict\n",
        "print(\"Model's state_dict:\")\n",
        "for param_tensor in model.state_dict():\n",
        "    print(param_tensor, \"\\t\", model.state_dict()[param_tensor].size())\n",
        "\n",
        "# Print optimizer's state_dict\n",
        "print(\"Optimizer's state_dict:\")\n",
        "for var_name in optimizer.state_dict():\n",
        "    print(var_name, \"\\t\", optimizer.state_dict()[var_name])\n",
        "\n",
        "\n",
        "# set up early stopping\n",
        "patience = 100\n",
        "early_stop = EarlyStopping(patience)\n",
        "\n",
        "# set up training schedule file\n",
        "sample_idx = 0   #index of the model being trained (zero-indexed)\n",
        "sched_filename = \"chembl_500000\" + str(sample_idx + 1) + \".csv\"\n",
        "sched_file = os.path.join(output_dir, sched_filename)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "max_epochs = 1000   #maximum number of epochs to train for\n",
        "gradient_clip = None, # type=float, amount to which to clip the gradients\n",
        "\n",
        "# manually deal with gradient clipping\n",
        "try:\n",
        "    gradient_clip = float(gradient_clip)\n",
        "except (ValueError, TypeError):\n",
        "    gradient_clip = None\n",
        "\n",
        "\n",
        "smiles_filename = \"sample-\" + str(3) + \"-SMILES.smi\"\n",
        "smiles_file = os.path.join(output_dir, smiles_filename)\n",
        "\n",
        "def training_model_rnn():\n",
        "    # iterate over epochs\n",
        "    counter = 0\n",
        "    for epoch in range(max_epochs):\n",
        "        # iterate over batches\n",
        "        for batch_idx, batch in tqdm(enumerate(loader), total=len(loader)):\n",
        "            batch, lengths = batch\n",
        "\n",
        "            # increment counter\n",
        "            counter += 1\n",
        "\n",
        "            # calculate loss\n",
        "            log_p = model.loss(batch, lengths)\n",
        "            loss = log_p.mean()\n",
        "\n",
        "            # zero gradients, calculate new gradients, and take a step\n",
        "            optimizer.zero_grad()\n",
        "            loss.backward()\n",
        "            # clip gradient\n",
        "            if gradient_clip is not None:\n",
        "                nn.utils.clip_grad_norm_(model.parameters(), gradient_clip)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # check learning rate decay\n",
        "            if learning_rate_decay is not None and \\\n",
        "                    counter % learning_rate_decay_steps == 0:\n",
        "                decrease_learning_rate(optimizer,\n",
        "                                      multiplier=learning_rate_decay)\n",
        "\n",
        "            # calculate validation loss\n",
        "            validation, lengths = dataset.get_validation(batch_size)\n",
        "            validation_loss = model.loss(validation, lengths).mean().detach()\n",
        "            # check early stopping\n",
        "            model_filename = \"model-chembl_500000\" + str(sample_idx + 1) + \".pth\" ##model filename\n",
        "            model_file = os.path.join(output_dir, model_filename)\n",
        "            early_stop(validation_loss.item(), model, model_file, counter)\n",
        "\n",
        "            if early_stop.stop:\n",
        "                break\n",
        "\n",
        "        # print update and write training schedule?\n",
        "        if log_every_epochs is not None:\n",
        "            #print_update(model, dataset, epoch, 'NA', loss.item(), batch_size)\n",
        "            track_loss(sched_file, model, dataset, epoch,\n",
        "                      counter, loss.item(), batch_size)\n",
        "\n",
        "        if early_stop.stop:\n",
        "            break\n",
        "\n",
        "    # append information about final training step\n",
        "    if log_every_epochs is not None:\n",
        "        sched = pd.DataFrame({'epoch': [None],\n",
        "                              'step': [early_stop.step_at_best],\n",
        "                              'outcome': ['training loss'],\n",
        "                              'value': [early_stop.best_loss]})\n",
        "        sched.to_csv(sched_file, index=False, mode='a', header=False)\n",
        "\n",
        "\n",
        "    # load the best model\n",
        "    model.load_state_dict(torch.load(model_file))\n",
        "    model.eval() ## enable evaluation modes\n",
        "\n",
        "    # sample a set of SMILES from the final, trained model\n",
        "    sampled_smiles = []\n",
        "    while len(sampled_smiles) < sample_size:\n",
        "        sampled_smiles.extend(model.sample(batch_size, return_smiles=True))\n",
        "\n",
        "    # write sampled SMILES\n",
        "    write_smiles(sampled_smiles, smiles_file)\n",
        "    #print(sampled_smiles)\n",
        "\n",
        "    def is_valid(smiles):\n",
        "      mol = Chem.MolFromSmiles(smiles)\n",
        "      if mol is not None and mol.GetNumAtoms()>0:\n",
        "         return smiles\n",
        "\n",
        "\n",
        "    mols = list(filter(is_valid,sampled_smiles)) \n",
        "     \n",
        "    print('Percentage of validity = ' + str((len(mols)/len(sampled_smiles))*100))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Toz-ITIa14jd"
      },
      "outputs": [],
      "source": [
        "## Save  vocab file    \n",
        "dataset.vocabulary.write('/content/gdrive/MyDrive/code/generator/pre-trained/vocab_chembl_500000_pat_50000')\n",
        "    "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PJCm14Yx14jd"
      },
      "outputs": [],
      "source": [
        "## training the model\n",
        "training_model_rnn()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9gE1a8-E14je"
      },
      "outputs": [],
      "source": [
        "## Save the model and vocab file\n",
        "\n",
        "def save_model(model, path):\n",
        "        torch.save(model.state_dict(), path)\n",
        "        \n",
        "path = '/content/gdrive/MyDrive/code/generator/pre-trained/checkpoint_chembl_500000_pat_50000'\n",
        "        \n",
        "save_model(model, path)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}